{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for comparing a base model (or supervised finetuned model) to its RLHF'd version\n",
    "Current goal: train an autoencoder from base activations to RLHF'd activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from collections.abc import Generator\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "import itertools\n",
    "import json\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import pickle\n",
    "from typing import Union, Tuple, List, Any, Optional, TypeVar, Dict\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from baukit import Trace\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from einops import rearrange\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtyping import TensorType\n",
    "from tqdm import tqdm\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformers import PreTrainedTokenizerBase, GPT2Tokenizer\n",
    "import wandb\n",
    "\n",
    "from utils import *\n",
    "from run_copy import *\n",
    "from argparser import parse_args\n",
    "from nanoGPT_model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleModule(nn.Module):\n",
    "    def __init__(self, n_dict_components, t_type=torch.float32):\n",
    "        super(ScaleModule, self).__init__()\n",
    "        self.scale_factor= nn.Parameter(torch.zeros(n_dict_components))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.scale_factor\n",
    "    \n",
    "    def scales(self):\n",
    "        return self.scale_factor.data\n",
    "        \n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, activation_size, n_dict_components, t_type=torch.float32, l1_coef=0.0):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        \n",
    "        # Only defining the decoder layer, encoder will share its weights\n",
    "        self.decoder = nn.Linear(n_dict_components, activation_size, bias=True)\n",
    "        # Create a bias layer\n",
    "        self.encoder_bias= nn.Parameter(torch.zeros(n_dict_components))\n",
    "        \n",
    "        # Add scaling term between encoder and decoder\n",
    "        self.scaling = ScaleModule(n_dict_components)\n",
    "\n",
    "        # Initialize the decoder weights orthogonally\n",
    "        nn.init.orthogonal_(self.decoder.weight)\n",
    "        self.decoder = self.decoder.to(t_type)\n",
    "\n",
    "        # Encoder is a Sequential with the ReLU activation\n",
    "        # No need to define a Linear layer for the encoder as its weights are tied with the decoder\n",
    "        self.encoder = nn.Sequential(nn.ReLU()).to(t_type)\n",
    "\n",
    "        self.l1_coef = l1_coef\n",
    "        self.activation_size = activation_size\n",
    "        self.n_dict_components = n_dict_components\n",
    "\n",
    "    def forward(self, x):\n",
    "        c = self.encoder(x @ self.decoder.weight + self.encoder_bias)\n",
    "        # Apply unit norm constraint to the decoder weights\n",
    "        self.decoder.weight.data = nn.functional.normalize(self.decoder.weight.data, dim=0)\n",
    "        \n",
    "        # scale encoder outputs\n",
    "        c = self.scaling(c)\n",
    "\n",
    "        # Decoding step as before\n",
    "        x_hat = self.decoder(c)\n",
    "        return x_hat, c\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to run the following terminal commands to create activation dictionaries:\n",
    "\n",
    "python3 run_copy.py --model_name [model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# model_name = \"usvsnsp/pythia-2.8b-sft\"\n",
    "# model_name = \"gpt2\"\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with Trace(model, \"h.0.mlp\") as ret:\n",
    "#     i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataset\n",
    "# dataset_folder = \"activation_data/pile-10k-gpt2-2\"\n",
    "# with open(os.path.join(dataset_folder, \"0.pkl\"), \"rb\") as f:\n",
    "#     dataset = pickle.load(f)\n",
    "#     mlp_width = dataset.tensors[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['foo']\n",
    "cfg = parse_args()\n",
    "cfg.model_name = \"gpt2\"\n",
    "cfg.load_activation_dataset = \"activation_data/pile-10k-gpt2-2\"\n",
    "cfg.dataset_folder = \"activation_data/pile-10k-gpt2-2\"\n",
    "cfg.n_components_dictionary = 512\n",
    "cfg.use_wandb = False\n",
    "cfg.mlp_width = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Coef: 1.00E-01 | Dict ratio: 0.6666666666666666 | Batch: 1000/1364 | Chunk: 1/4 | Minirun: 2/1 | Epoch: 1/1 | Reconstruction loss: 0.993337 | l1: 0.000001\n",
      "L1 Coef: 1.00E-01 | Dict ratio: 0.6666666666666666 | Batch: 636/1364 | Chunk: 2/4 | Minirun: 2/1 | Epoch: 1/1 | Reconstruction loss: 0.946200 | l1: 0.000000\n",
      "L1 Coef: 1.00E-01 | Dict ratio: 0.6666666666666666 | Batch: 272/1364 | Chunk: 3/4 | Minirun: 2/1 | Epoch: 1/1 | Reconstruction loss: 0.908071 | l1: 0.000000\n",
      "L1 Coef: 1.00E-01 | Dict ratio: 0.6666666666666666 | Batch: 1272/1364 | Chunk: 3/4 | Minirun: 2/1 | Epoch: 1/1 | Reconstruction loss: 0.874478 | l1: 0.000000\n",
      "L1 Coef: 1.00E-01 | Dict ratio: 0.6666666666666666 | Batch: 908/1364 | Chunk: 4/4 | Minirun: 2/1 | Epoch: 1/1 | Reconstruction loss: 0.849065 | l1: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# train EncoderDecoder\n",
    "from run_copy import *\n",
    "# from run_copy import run_different_target\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "t_type = torch.float32\n",
    "l1_alpha = 0.1\n",
    "\n",
    "\n",
    "\n",
    "auto_encoder = EncoderDecoder(cfg.mlp_width, cfg.n_components_dictionary, t_type, l1_coef=l1_alpha).to(device)\n",
    "auto_encoder, reconstruction_loss, l1_loss, feature_activations, completed_batches = run_different_target(cfg, auto_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0161, device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_encoder.scaling.scale_factor.data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
